# The Spec Runner
Pending Specs
You can mark an individual spec or container as Pending. This will prevent the spec (or specs within the container) from running. You do this by adding a P or an X in front of your Describe, Context, It, and Measure:
````
PDescribe("some behavior", func() { ... })
PContext("some scenario", func() { ... })
PIt("some assertion")
PMeasure("some measurement")

XDescribe("some behavior", func() { ... })
XContext("some scenario", func() { ... })
XIt("some assertion")
XMeasure("some measurement")
````
You don’t need to remove the func() { ... } when you mark an It or Measure as pending. Ginkgo will happily ignore any arguments after the string.
By default, Ginkgo will print out a description for each pending spec. You can suppress this by setting the --noisyPendings=false flag.
By default, Ginkgo will not fail a suite for having pending specs. You can pass the --failOnPending flag to reverse this behavior.
Using the P and X prefixes marks specs as pending at compile time. If you need to skip a spec at runtime (perhaps due to a constraint that can only be known at runtime) you may call Skip in your test:
````
It("should do something, if it can", func() {
    if !someCondition {
        Skip("special condition wasn't met")
    }

    // assertions go here
})
````
Note that Skip(...) causes the closure to exit so there is no need to return.

## Focused Specs
It is often convenient, when developing to be able to run a subset of specs. Ginkgo has two mechanisms for allowing you to focus specs:

You can focus individual specs or whole containers of specs programatically by adding an F in front of your Describe, Context, and It:
````
 FDescribe("some behavior", func() { ... })
 FContext("some scenario", func() { ... })
 FIt("some assertion", func() { ... })
 ````
doing so instructs Ginkgo to only run those specs. To run all specs, you’ll need to go back and remove all the Fs.

You can pass in a regular expression with the --focus=REGEXP and/or --skip=REGEXP flags. Ginkgo will only run specs that match the focus regular expression and don’t match the skip regular expression.

In cases where specs dont provide enough hierarchichal distinction between groups of tests, directories can be included in the matching of focus and skip, via the --regexScansFilePath option. That is, if the original code location for a test is test/a/b/c/my_test.go, one can combine --focus=/b/ along with --regexScansFilePath=true to focus on tests including the path /b/. This feature is useful for filtering tests in binary artifacts along the lines of the original directory where those tests were created - but ideally your specs should be organized in such a way as to minimize the need for using this feature.
When Ginkgo detects that a passing test suite has a programmatically focused test it causes the suite to exit with a non-zero status code. This is to help detect erroneously committed focused tests on CI systems. When passed a command-line focus/skip flag Ginkgo exits with status code 0 - if you want to focus tests on your CI system you should explicitly pass in a -focus or -skip flag.

Nested programmatically focused specs follow a simple rule: if a leaf-node is marked focused, any of its ancestor nodes that are marked focus will be unfocused. With this rule, sibling leaf nodes (regardless of relative-depth) that are focused will run regardless of the focus of a shared ancestor; and non-focused siblings will not run regardless of the focus of the shared ancestor or the relative depths of the siblings. More simply:
````
FDescribe("outer describe", func() {
    It("A", func() { ... })
    It("B", func() { ... })
})
````
will run both Its but
````
FDescribe("outer describe", func() {
    It("A", func() { ... })
    FIt("B", func() { ... })
})
````
will only run B. This behavior tends to map more closely to what the developer actually intends when iterating on a test suite.

The programatic approach and the --focus=REGEXP/--skip=REGEXP approach are mutually exclusive. Using the command line flags will override the programmatic focus.
Focusing a container with no It or Measure leaf nodes has no effect. Since there is nothing to run in the container, Ginkgo effectively ignores it.
When using the command line flags you can specify one or both of --focus and --skip. If both are specified the constraints will be ANDed together.
You can unfocus programatically focused tests by running ginkgo unfocus. This will strip the Fs off of any FDescribe, FContext, and FIts that your tests in the current directory may have.
If you want to skip entire packages (when running ginkgo recursively with the -r flag) you can pass a comma-separated list to --skipPackage=PACKAGES,TO,SKIP. Any packages with paths that contain one of the entries in this comma separated list will be skipped.
Spec Permutation
By default, Ginkgo will randomize the order in which your specs are run. This can help suss out test pollution early on in a suite’s development.

Ginkgo’s default behavior is to only permute the order of top-level containers – the specs within those containers continue to run in the order in which they are specified in the test file. This is helpful when developing specs as it mitigates the coginitive overload of having specs continuously change the order in which they run.

To randomize all specs in a suite, you can pass the --randomizeAllSpecs flag. This is useful on CI and can greatly help fight the scourge of test pollution.

Ginkgo uses the current time to seed the randomization. It prints out the seed near the beginning of the test output. If you notice test intermittent test failures that you think may be due to test pollution, you can use the seed from a failing suite to exactly reproduce the spec order for that suite. To do this pass the --seed=SEED flag.

When running multiple spec suites Ginkgo defaults to running the suites in the order they would be listed on the file-system. You can permute the suites by passing ginkgo --randomizeSuites

Parallel Specs
Ginkgo has support for running specs in parallel. It does this by spawning separate go test processes and serving specs to each process off of a shared queue. This is important for a BDD test framework, as the shared context of the closures does not parallelize well in-process.

To run a Ginkgo suite in parallel you must use the ginkgo CLI. Simply pass in the -p flag:
````
ginkgo -p
````
this will automatically detect the optimal number of test nodes to spawn (see the note below).

To specify the number of nodes to spawn, use -nodes:
````
ginkgo -nodes=N
````
You do not need to specify both -p and -nodes. Setting -nodes to anything greater than 1 implies a parallelized test run.
The number of nodes used with -p is runtime.NumCPU() if runtime.NumCPU() <= 4, otherwise it is runtime.NumCPU() - 1 based on a rigorous science based heuristic best characterized as “my gut sense based on a few months of experience”
The test runner collates output from the running processes into one coherent output. This is done, under the hood, via a client-server model: as each client suite completes a test, the test output and status is sent to the server which then prints to screen. This collates the output of simultaneous test runners to one coherent (i.e. non-interleaved), aggregated, output.

It is sometimes necessary/preferable to view the output of the individual parallel test suites in real-time. To do this you can set -stream:
````
ginkgo -nodes=N -stream
````
When run with the -stream flag the test runner simply pipes the output from each individual node as it runs (it prepends each line of output with the node # that the output came from). This results in less coherent output (lines from different nodes will be interleaved) but can be valuable when debugging flakey/hanging test suites.

On windows, parallel tests default to -stream because Ginkgo can’t capture logging to stdout/stderr (necessary for aggregation) on windows.
Managing External Processes in Parallel Test Suites

If your tests spin up or connect to external processes you’ll need to make sure that those connections are safe in a parallel context. One way to ensure this would be, for example, to spin up a separate instance of an external resource for each Ginkgo process. For example, let’s say your tests spin up and hit a database. You could bring up a different database server bound to a different port for each of your parallel processes:
````
package books_test

import (
    . "github.com/onsi/ginkgo"
    . "github.com/onsi/gomega"
    "github.com/onsi/ginkgo/config"

    "your/db"

    "testing"
)

var dbRunner *db.Runner
var dbClient *db.Client


func TestBooks(t *testing.T) {
    RegisterFailHandler(Fail)

    RunSpecs(t, "Books Suite")
}

var _ = BeforeSuite(func() {
    port := 4000 + config.GinkgoConfig.ParallelNode

    dbRunner = db.NewRunner()
    err := dbRunner.Start(port)
    Expect(err).NotTo(HaveOccurred())

    dbClient = db.NewClient()
    err = dbClient.Connect(dbRunner.Address())
    Expect(err).NotTo(HaveOccurred())
})

var _ = AfterSuite(func() {
    dbClient.Cleanup()
    dbRunner.Stop()
})
````
The github.com/onsi/ginkgo/config package provides your suite with access to the command line configuration parameters passed into Ginkgo. The config.GinkgoConfig.ParallelNode parameter is the index for the current node (starts with 1, goes up to N). Similarly config.GinkgoConfig.ParallelTotal is the total number of nodes running in parallel.

## Managing Singleton External Processes in Parallel Test Suites

When possible, you should make every effort to start up a new instance of an external resource for every parallel node. This helps avoid test-pollution by strictly separating each parallel node.

Sometimes (rarely) this is not possible. Perhaps, for reasons beyond your control, you can only start one instance of a service on your machine. Ginkgo provides a workaround for this with SynchronizedBeforeSuite and SynchronizedAfterSuite.

The idea here is simple. With SynchronizedBeforeSuite Ginkgo gives you a way to run some preliminary setup code on just one parallel node (Node 1) and other setup code on all nodes. Ginkgo synchronizes these functions and guarantees that node 1 will complete its preliminary setup before the other nodes run their setup code. Moreover, Ginkgo makes it possible for the preliminary setup code on the first node to pass information on to the setup code on the other nodes.

Here’s what our earlier database example looks like using SynchronizedBeforeSuite:
````
var _ = SynchronizedBeforeSuite(func() []byte {
    port := 4000 + config.GinkgoConfig.ParallelNode

    dbRunner = db.NewRunner()
    err := dbRunner.Start(port)
    Expect(err).NotTo(HaveOccurred())

    return []byte(dbRunner.Address())
}, func(data []byte) {
    dbAddress := string(data)

    dbClient = db.NewClient()
    err = dbClient.Connect(dbAddress)
    Expect(err).NotTo(HaveOccurred())
})
````
SynchronizedBeforeSuite must be passed two functions. The first must return []byte and the second must accept []byte. When running with multiple nodes the first function is only run on node 1. When this function completes, all nodes (including node 1) proceed to run the second function and will receive the data returned by the first function. In this example, we use this data-passing mechanism to forward the database’s address (set up on node 1) to all nodes.

To clean up correctly, you should use SynchronizedAfterSuite. Continuing our example:
````
var _ = SynchronizedAfterSuite(func() {
    dbClient.Cleanup()
}, func() {
    dbRunner.Stop()
})
````
With SynchronizedAfterSuite the first function is run on all nodes (including node 1). The second function is only run on node 1. Moreover, the second function is only run when all other nodes have finished running. This is important, since node 1 is responsible for setting up and tearing down the singleton resources it must wait for the other nodes to end before tearing down the resources they depend on.

Finally, all of these function can be passed an additional Done parameter to run asynchronously. When running asynchronously, an optional timeout can be provided as a third parameter to SynchronizedBeforeSuite and SynchronizedAfterSuite. The same timeout is applied to both functions.

Note an important subtelty: The dbRunner variable is only populated on Node 1. No other node should attempt to touch the data in that variable (it will be nil on the other nodes). The dbClient variable, which is populated in the second SynchronizedBeforeSuite function is, of course, available across all nodes.
